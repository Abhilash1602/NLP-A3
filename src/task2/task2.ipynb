{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "b3384613",
   "metadata": {},
   "source": [
    "# (Q2) Task - Claim Normalization\n",
    "\n",
    "Claim Normalization is a novel task that involves transforming complex and noisy social media posts into clear and structured claims, referred to as normalized claims. This task will be evaluated using the CLAN dataset, which comprises of real-world social media posts along with their corresponding normalized claims. You can refer to the paper here for more details.\n",
    "\n",
    "## (2.1) - Dataset Description\n",
    "Students will be provided with the CLAN data.csv file, which contains social media posts along with their corresponding normalized claims, annotated by a professional fact-checker. The dataset should be divided into training, validation, and test sets in a 70-15-15 ratio for model training and evaluation."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "7e4fcdfa",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import os\n",
    "import sys\n",
    "BASE_DIR = os.getcwd()\n",
    "DATA_DIR = os.path.join(BASE_DIR, '../../dataset/task2/CLAN_data.csv')\n",
    "RANDOM_SEED = 42\n",
    "clan_data = pd.read_csv(DATA_DIR, encoding='utf-8')\n",
    "\n",
    "# Splitting the data into train, val and test sets. Ratio: 70:15:15\n",
    "def split_data(data, train_size=0.7, val_size=0.15, test_size=0.15):\n",
    "    assert train_size + val_size + test_size == 1, \"Train, val and test sizes must sum to 1\"\n",
    "    train_data = data.sample(frac=train_size, random_state=RANDOM_SEED)\n",
    "    remaining_data = data.drop(train_data.index)\n",
    "    val_data = remaining_data.sample(frac=val_size/(val_size + test_size), random_state=RANDOM_SEED)\n",
    "    test_data = remaining_data.drop(val_data.index)\n",
    "    return train_data, val_data, test_data"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b743de20",
   "metadata": {},
   "source": [
    "## (2.2) - Preprocessing\n",
    "\n",
    "Students need to preprocess the social media posts before claim normalization. The following steps should be included:\n",
    "-  Expand contractions and abbreviations: Replace common contractions and abbreviations with their expanded forms (e.g., he’ll → he will, she’s → she is, Gov. → Governor, Feb. → February, VP → Vice President, ETA → Estimated Time of Arrival).\n",
    "- Clean text: Remove links, special characters, and extra whitespace while converting text to lowercase."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "aa0040e2",
   "metadata": {},
   "outputs": [],
   "source": [
    "import re\n",
    "from contractions import contractions_dict\n",
    "\n",
    "'''\n",
    "Preprocessing: We have three columns in the dataset: (['PID', 'Social Media Post', 'Normalized Claim'], dtype='object')\n",
    "- PID: Integer number representing the post ID\n",
    "- Social Media Post: The text of the post\n",
    "- Normalized Claim: The claim in the post, which is a string of text\n",
    "'''\n",
    "\n",
    "def expand_contractions(text, contractions_dict):\n",
    "    \"\"\"\n",
    "    Expands contractions in the given text using the provided contractions dictionary.\n",
    "    \"\"\"\n",
    "    pattern = re.compile(r'\\b(' + '|'.join(re.escape(key) for key in contractions_dict.keys()) + r')\\b')\n",
    "    return pattern.sub(lambda x: contractions_dict[x.group()], text)\n",
    "\n",
    "def clean_text(text):\n",
    "    \"\"\"\n",
    "    Cleans the text by removing links, special characters, and extra whitespace, and converts to lowercase.\n",
    "    \"\"\"\n",
    "    # Remove URLs\n",
    "    text = re.sub(r'http\\S+|www\\S+|https\\S+', '', text, flags=re.MULTILINE)\n",
    "    # Remove special characters and numbers (except spaces)\n",
    "    text = re.sub(r'[^a-zA-Z\\s]', '', text)\n",
    "    # Convert to lowercase\n",
    "    text = text.lower()\n",
    "    # Remove extra whitespace\n",
    "    text = re.sub(r'\\s+', ' ', text).strip()\n",
    "    return text\n",
    "\n",
    "def preprocess_data(data):\n",
    "    \"\"\"\n",
    "    Preprocesses the dataset by expanding contractions and cleaning text.\n",
    "    \"\"\"\n",
    "    # Apply preprocessing to the 'Social Media Post' column\n",
    "    data['Social Media Post'] = data['Social Media Post'].apply(lambda x: expand_contractions(x, contractions_dict))\n",
    "    data['Social Media Post'] = data['Social Media Post'].apply(clean_text)\n",
    "    data['Normalized Claim'] = data['Normalized Claim'].apply(lambda x: expand_contractions(x, contractions_dict))\n",
    "    data['Normalized Claim'] = data['Normalized Claim'].apply(clean_text)\n",
    "    return data\n",
    "\n",
    "clean_data = clan_data.copy()\n",
    "# Preprocess the data\n",
    "clean_data = preprocess_data(clean_data)\n",
    "# Split the data into train, val and test sets\n",
    "train_data, val_data, test_data = split_data(clean_data)\n",
    "\n",
    "# Reset index for all datasets\n",
    "train_data.reset_index(drop=True, inplace=True)\n",
    "val_data.reset_index(drop=True, inplace=True)\n",
    "test_data.reset_index(drop=True, inplace=True)\n",
    "\n",
    "# Save the cleaned data to CSV files\n",
    "clean_data.to_csv(os.path.join(BASE_DIR, '../../dataset/task2/cleaned_data.csv'), index=False)\n",
    "\n",
    "# Save the cleaned data to CSV files\n",
    "train_data.to_csv(os.path.join(BASE_DIR, '../../dataset/task2/train_data.csv'), index=False)\n",
    "val_data.to_csv(os.path.join(BASE_DIR, '../../dataset/task2/val_data.csv'), index=False)\n",
    "test_data.to_csv(os.path.join(BASE_DIR, '../../dataset/task2/test_data.csv'), index=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b767ca64",
   "metadata": {},
   "source": [
    "## (2.3) - Model Training\n",
    "\n",
    "You are required to train both BART and T5 models by fine-tuning them on the provided dataset. The choice of BART and T5 variants is left to your discretion, with the goal of achieving the best possible results while optimizing for limited GPU resources available in Google Colab or Kaggle Notebooks.\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "60c3d078",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "base",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
